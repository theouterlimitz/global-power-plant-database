{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z8bovuDYyxL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Capstone Deliverable: Data Curation\n",
        "\n",
        "This notebook documents the process of acquiring, cleaning, merging, and preparing the datasets for the \"Assessing Water Risk Exposure of Global Power Plant Infrastructure\" project.\n",
        "\n",
        "The process involves these major steps:\n",
        "\n",
        "Setup: Cloning the project repository and unzipping geospatial data.\n",
        "\n",
        "Loading Raw Data: Reading the power plant database, Aqueduct baseline and future risk data, and the geospatial basin data.\n",
        "\n",
        "Data Merging: Performing a geospatial join to link power plants to water basins, followed by attribute joins to add the detailed risk data.\n",
        "\n",
        "Data Cleaning: Handling identified data quality issues like special numeric codes, missing values, and anomalies.\n",
        "\n",
        "Saving the Final Dataset: Exporting the final, clean, and analysis-ready dataset to a Pickle file to streamline all future analysis.\n",
        "\n",
        "Datasets Used:\n",
        "\n",
        "WRI Global Power Plant Database (GPPD)\n",
        "WRI Aqueduct 4.0 Baseline Monthly & Future Annual Data\n",
        "WRI Aqueduct 4.0 Geospatial Data (File Geodatabase)"
      ],
      "metadata": {
        "id": "ImQ9qGahYzj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 2: SETUP AND ENVIRONMENT\n",
        "#\n",
        "# This cell imports all necessary libraries and prepares the\n",
        "# Colab environment by cloning the GitHub repository and unzipping\n",
        "# the necessary geospatial data.\n",
        "# ===================================================================\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import fiona\n",
        "import numpy as np\n",
        "\n",
        "# --- Environment Setup ---\n",
        "\n",
        "# Clone the GitHub repository if it doesn't already exist in the session\n",
        "repo_path = 'global-power-plant-database'\n",
        "if not os.path.exists(repo_path):\n",
        "    print(\"Cloning GitHub repository...\")\n",
        "    # This command downloads your project repository into the Colab environment\n",
        "    !git clone https://github.com/theouterlimitz/global-power-plant-database.git\n",
        "else:\n",
        "    print(\"GitHub repository already exists.\")\n",
        "\n",
        "# Unzip the Geodatabase file if it hasn't been unzipped yet\n",
        "# IMPORTANT: Make sure you have uploaded your zipped GDB file (e.g., 'GDB.zip') first!\n",
        "gdb_folder_path = 'GDB'\n",
        "if not os.path.exists(gdb_folder_path):\n",
        "    print(\"\\nUnzipping geospatial data...\")\n",
        "    # This command unzips the geodatabase folder.\n",
        "    # Replace 'GDB.zip' with the actual name of your zip file if different.\n",
        "    !unzip GDB.zip\n",
        "else:\n",
        "    print(\"\\nGeospatial data folder already exists.\")"
      ],
      "metadata": {
        "id": "Yk27d39lZOWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Loading\n",
        "\n",
        "This step loads all raw data sources from their respective files into pandas or geopandas DataFrames. This includes the power plant database, the two tabular Aqueduct risk datasets, and the geospatial basin data from the File Geodatabase.\n"
      ],
      "metadata": {
        "id": "IwOlK3BHZcsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 4: LOAD ALL RAW DATA SOURCES\n",
        "#\n",
        "# This cell reads all necessary files into memory.\n",
        "# It assumes you have uploaded the two Aqueduct CSVs and the GDB.zip\n",
        "# file to your Colab session.\n",
        "# ===================================================================\n",
        "print(\"\\n--- Loading All Raw Datasets ---\")\n",
        "\n",
        "try:\n",
        "    # Load Power Plant Data from the cloned GitHub repository\n",
        "    gppd_file_path = 'global-power-plant-database/output_database/global_power_plant_database.csv'\n",
        "    df_power_plants = pd.read_csv(gppd_file_path, low_memory=False)\n",
        "    print(\"Successfully loaded: Global Power Plant Database\")\n",
        "\n",
        "    # Load Aqueduct Baseline Monthly Data from the uploaded CSV\n",
        "    aq_baseline_monthly_path = 'Aqueduct40_baseline_monthly_y2023m07d05.csv'\n",
        "    df_aq_baseline_monthly = pd.read_csv(aq_baseline_monthly_path)\n",
        "    print(\"Successfully loaded: Aqueduct Baseline Monthly Data\")\n",
        "\n",
        "    # Load Aqueduct Future Annual Data from the uploaded CSV\n",
        "    aq_future_annual_path = 'Aqueduct40_future_annual_y2023m07d05.csv'\n",
        "    df_aq_future_annual = pd.read_csv(aq_future_annual_path)\n",
        "    print(\"Successfully loaded: Aqueduct Future Annual Data\")\n",
        "\n",
        "    # Load the geospatial basin layer from the unzipped File Geodatabase\n",
        "    gdb_path = 'GDB/Aq40_Y2023D07M05.gdb'\n",
        "    target_layer_name = 'baseline_annual'\n",
        "    gdf_basins = gpd.read_file(gdb_path, layer=target_layer_name)\n",
        "    print(\"Successfully loaded: Geospatial basin data\")\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"\\nERROR: A file was not found. {e}\")\n",
        "    print(\"Please ensure you have uploaded the two Aqueduct CSVs and your GDB.zip file to this Colab session.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during data loading: {e}\")"
      ],
      "metadata": {
        "id": "1unZax1cZivV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Merging\n",
        "\n",
        "This section contains the core data integration logic. First, the detailed basin polygons are simplified. Second, a geospatial join is performed to assign a basin ID (pfaf_id) to each power plant based on its location. Finally, standard attribute joins are used to merge the detailed baseline and future water risk data onto the power plants using the common pfaf_id.\n",
        "\n"
      ],
      "metadata": {
        "id": "V31IULNlZoTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 6: DATA MERGING (GEOSPATIAL AND ATTRIBUTE JOINS)\n",
        "# ===================================================================\n",
        "print(\"\\n--- Merging All Datasets ---\")\n",
        "\n",
        "# --- Step 6.1: Simplify Basin Geometries ---\n",
        "# The original basin file is split by province/aquifer. We dissolve it\n",
        "# to get one single, complete shape for each unique water basin (pfaf_id).\n",
        "print(\"Simplifying basin geometries...\")\n",
        "basins_simplified = gdf_basins[['pfaf_id', 'geometry']].dissolve(by='pfaf_id', aggfunc='first').reset_index()\n",
        "\n",
        "# --- Step 6.2: Geospatial Join ---\n",
        "# Convert the power plant DataFrame to a GeoDataFrame by creating points from lat/lon\n",
        "print(\"Converting power plants DataFrame to a GeoDataFrame...\")\n",
        "gdf_power_plants = gpd.GeoDataFrame(\n",
        "    df_power_plants,\n",
        "    geometry=gpd.points_from_xy(df_power_plants.longitude, df_power_plants.latitude),\n",
        "    crs=\"EPSG:4326\"  # Set CRS to WGS84 (standard lat/lon)\n",
        ")\n",
        "\n",
        "# Perform the spatial join to find which basin each power plant point is 'within'\n",
        "print(\"Performing spatial join...\")\n",
        "gdf_plants_with_basins = gpd.sjoin(gdf_power_plants, basins_simplified, how=\"left\", predicate=\"within\")\n",
        "print(\"Spatial join complete.\")\n",
        "\n",
        "# --- Step 6.3: Attribute Joins ---\n",
        "# Now, merge the tabular water risk data using the common 'pfaf_id' column\n",
        "print(\"Merging baseline and future risk data...\")\n",
        "df_merged = pd.merge(gdf_plants_with_basins, df_aq_baseline_monthly, on='pfaf_id', how='left')\n",
        "df_final = pd.merge(df_merged, df_aq_future_annual, on='pfaf_id', how='left')\n",
        "print(\"Attribute joins complete.\")"
      ],
      "metadata": {
        "id": "-LNlxOHcZwUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Data Cleaning\n",
        "\n",
        "This section addresses the data quality issues identified during our initial review. This includes handling special numeric codes used for \"No Data\" and clipping anomalous negative values to ensure the data is clean and ready for analysis."
      ],
      "metadata": {
        "id": "UHZzbqhYZzky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 8: FINAL DATA CLEANING\n",
        "# ===================================================================\n",
        "print(\"\\n--- Performing Final Data Cleaning ---\")\n",
        "\n",
        "# Replace all instances of -9999 (the \"No Data\" code) with NumPy's Not a Number (np.nan)\n",
        "# This ensures missing data is handled correctly by pandas and plotting libraries.\n",
        "print(\"Replacing -9999 values with np.nan...\")\n",
        "df_final.replace(-9999, np.nan, inplace=True)\n",
        "\n",
        "# Address the negative reported generation values. We cap them at 0 as negative generation is not physically meaningful here.\n",
        "gen_gwh_cols = [col for col in df_final.columns if 'generation_gwh_' in col and 'estimated' not in col]\n",
        "print(\"Capping negative reported generation values at 0...\")\n",
        "for col in gen_gwh_cols:\n",
        "    df_final[col] = df_final[col].clip(lower=0)\n",
        "\n",
        "# Drop the unnecessary 'index_right' column that was created during the spatial join\n",
        "if 'index_right' in df_final.columns:\n",
        "    df_final.drop(columns=['index_right'], inplace=True)\n",
        "print(\"Dropped unnecessary columns.\")\n",
        "\n",
        "print(\"\\nFinal cleaning complete!\")"
      ],
      "metadata": {
        "id": "qO8YIurBZ5Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save Final Analytical Dataset\n",
        "\n",
        "The Data Curation process is complete. The final, merged, and cleaned GeoDataFrame is saved to a Pickle (.pkl) file. This format is efficient and preserves all data types (including the geometry column). By saving this file, all future analysis can begin by simply loading this single file, avoiding the need to re-run this entire pipeline."
      ],
      "metadata": {
        "id": "i_uMhDCIaAYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# CELL 10: SAVE AND INSPECT THE FINAL DATASET\n",
        "# ===================================================================\n",
        "print(\"\\n--- Saving Final Analytical Dataset ---\")\n",
        "\n",
        "# Define the output filename\n",
        "output_filename = 'analytical_data.pkl'\n",
        "\n",
        "# Save the final DataFrame to a Pickle file\n",
        "df_final.to_pickle(output_filename)\n",
        "\n",
        "print(f\"Successfully saved the final, cleaned dataset to '{output_filename}'\")\n",
        "print(\"\\nData Curation phase is complete!\")\n",
        "\n",
        "# --- Final Inspection ---\n",
        "print(\"\\n--- Final Dataset Overview ---\")\n",
        "print(\"\\nInfo for the final, merged DataFrame:\")\n",
        "df_final.info(verbose=False)  # Using verbose=False for a concise summary\n",
        "\n",
        "print(\"\\nShape of the final DataFrame (rows, columns):\")\n",
        "print(df_final.shape)\n",
        "\n",
        "print(\"\\nSample of key columns from the final dataset:\")\n",
        "# Display a small subset of columns to verify the final structure\n",
        "key_cols_to_display = ['name', 'country', 'primary_fuel', 'capacity_mw', 'pfaf_id', 'bws_01_label', 'bau80_ws_s']\n",
        "print(df_final[key_cols_to_display].head())"
      ],
      "metadata": {
        "id": "_7M6MHh4aCm8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}